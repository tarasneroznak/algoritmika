a) напевно найкращим варіантом буде лінійно пройти по дата сету і свопати елемент поки він меньший
якщо взяти сортування то в складність буде n log n
також, наприклад, якщо шукати бінарним пошуком то це дасть приросту так як місце для вставки теж потрібно знайти тому складність буде n log n

тому найкращим варіантом буде - лінійно
або наприклад можливо застосувати техніку TwoPointer та йти з двох сторін датасету до центру (не впевнений не тестував)

1, data1
2, data1
3, data1
5, data1 - bad
4, data1
7, data1 - bad
6, data1
9, data1 - bad
8, data1

def resolve_collision(data):
   for i in range(len(data) - 1):
      if (data[i] > data[i + 1]):
         data[i], data[i + 1] = data[i + 1], data[i]


b) 
n = 10000
k = 4000
m = 6000

перший варіант буде найкращим по кількості операцій для відсортованих даних
пошук по всім 10000 займатиме log n і якщо просто порахувати кількість операцій то виходить
що для 10000 зробимо порядка 13 порівнянь
в випадку розділення масиву log n від 4000 рівний 10 що не набагато більше пошуку в цілому масиві
а якщо ділити на дві частини то в 40% відсотках випадків кількість операцій = O(log k) + O(log m) = 10 + 12 = 22

для випадку з невідсортованими даними
навпаки краще виконувати пошук спочатку для хороших клієнтів
так вийде що в 60% складність буде O(k)
і в 40% випадків O(n) 
